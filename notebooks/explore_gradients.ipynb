{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "explore_gradients.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiAtArmMwTHh",
        "colab_type": "text"
      },
      "source": [
        "# Exploring gradients with Weights & Biases\n",
        "\n",
        "First we have to install a couple additional packages into the Colab runtime for the training loop to work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgLBTJuJDvau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install wandb tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Xm9CqYwocx",
        "colab_type": "text"
      },
      "source": [
        "Then you should log in with your Weights & Biases API key found [here](https://app.wandb.ai/authorize) to allow logging. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9ZoF4niJC8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5A1Ko1vx-Gk",
        "colab_type": "text"
      },
      "source": [
        "## Defining our model and train/test loops\n",
        "\n",
        "Here we'll set up the code we need to run a couple of different model to classify the [MNIST digits](http://yann.lecun.com/exdb/mnist/) dataset. We'll be borrowing a lot of the boilerplate code from the PyTorch MNIST example found [here](https://github.com/pytorch/examples/blob/master/mnist/main.py).\n",
        "\n",
        "\n",
        "Our model has two major components that will illustrate a couple different advantages of tracking gradients while training a deep learning model. The first component is a pretty basic 2D CNN --> fully-connected model that will do the heavy lifting of making the actual prediction. The second part feeds in 10 random values, passes them through a fully connected layer and concatenates them to the flattened output of the second 2D CNN layer. These random parameters carry no real value for the prediction task at hand. Check out how the gradients flowing to these parameters (which appear as `gradients/rand_fc.weight` and `gradients/rand_fc.weight` in your W&D dashboard) compare to those of the other model parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PbI0Jq4I5wP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import wandb\n",
        "from tqdm import *\n",
        "\n",
        "\n",
        "class CNN_Net(nn.Module):\n",
        "    def __init__(self, device):\n",
        "        super(CNN_Net, self).__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.rand_fc = nn.Linear(10, 10)\n",
        "        self.fc1 = nn.Linear((4*4*50) + 10, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        rand_x = torch.randn(x.shape[0], 10).to(self.device)\n",
        "        \n",
        "        rand_x = F.relu(self.rand_fc(rand_x))\n",
        "      \n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        \n",
        "        x = torch.cat((x, rand_x), dim=1)\n",
        "        \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "      \n",
        "    \n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    \n",
        "    n_ex = len(train_loader)\n",
        "    \n",
        "    for batch_idx, (data, target) in tqdm(enumerate(train_loader), total=n_ex):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        \n",
        "def test(model, device, test_loader, WANDB):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() \n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    tqdm.write('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "    if WANDB:\n",
        "        wandb.log({'test_loss': test_loss,\n",
        "                   'accuracy': correct / len(test_loader.dataset)})\n",
        "        \n",
        "        \n",
        "def main(config):\n",
        "    \n",
        "    if config['WANDB']:\n",
        "        wandb.init(project='explore-gradients', reinit=True, config=config)\n",
        "  \n",
        "    use_cuda = torch.cuda.is_available()\n",
        "\n",
        "    torch.manual_seed(config['SEED'])\n",
        "\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.1307,), (0.3081,))\n",
        "                       ])),\n",
        "        batch_size=config['BATCH_SIZE'], shuffle=True, **kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.1307,), (0.3081,))\n",
        "                       ])),\n",
        "        batch_size=config['TEST_BATCH_SIZE'], shuffle=True, **kwargs)\n",
        "\n",
        "    model = CNN_Net(device).to(device)\n",
        "    \n",
        "    \n",
        "    if config['WANDB']:\n",
        "        wandb.watch(model, log='all')\n",
        "    \n",
        "    optimizer = optim.SGD(model.parameters(),\n",
        "                          lr=config['LR'],\n",
        "                          momentum=config['MOMENTUM'])\n",
        "\n",
        "    for epoch in range(1, config['EPOCHS'] + 1):\n",
        "        train(model, device, train_loader, optimizer, epoch)\n",
        "        test(model, device, test_loader, config['WANDB'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN4SvuH_88YB",
        "colab_type": "text"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "Here you can edit the configuration dictionary to see how changing hyperparameters like the learning rate or momentum affect the gradients. If you want to turn off W&B experiment tracking, set `WANDB` to `False`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DhhZZO405jj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'BATCH_SIZE'         : 64,\n",
        "    'TEST_BATCH_SIZE'    : 1000,\n",
        "    'EPOCHS'             : 30,\n",
        "    'LR'                 : 0.01,\n",
        "    'MOMENTUM'           : 0,\n",
        "    'SEED'               : 17,\n",
        "    'WANDB'              : True,\n",
        "}\n",
        "\n",
        "main(config=config)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}